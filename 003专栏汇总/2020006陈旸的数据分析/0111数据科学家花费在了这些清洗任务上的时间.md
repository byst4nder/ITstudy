# 0111 数据科学家 80% 时间都花费在了这些清洗任务上？

陈旸 2019-01-07




09:45


讲述：陈旸 大小：8.94M

我们在上一节中讲了数据采集，以及相关的工具使用，但做完数据采集就可以直接进行挖掘了吗？肯定不是的。

就拿做饭打个比方吧，对于很多人来说，热油下锅、掌勺翻炒一定是做饭中最过瘾的环节，但实际上炒菜这个过程只占做饭时间的 20%，剩下 80% 的时间都是在做准备，比如买菜、择菜、洗菜等等。

在数据挖掘中，数据清洗就是这样的前期准备工作。对于数据科学家来说，我们会遇到各种各样的数据，在分析前，要投入大量的时间和精力把数据「整理裁剪」成自己想要或需要的样子。

为什么呢？因为我们采集到的数据往往有很多问题。

我们先看一个例子，假设老板给你以下的数据，让你做数据分析，你看到这个数据后有什么感觉呢？

你刚看到这些数据可能会比较懵，因为这些数据缺少标注。

我们在收集整理数据的时候，一定要对数据做标注，数据表头很重要。比如这份数据表，就缺少列名的标注，这样一来我们就不知道每列数据所代表的含义，无法从业务中理解这些数值的作用，以及这些数值是否正确。但在实际工作中，也可能像这个案例一样，数据是缺少标注的。

我简单解释下这些数据代表的含义。

这是一家服装店统计的会员数据。最上面的一行是列坐标，最左侧一列是行坐标。

列坐标中，第 0 列代表的是序号，第 1 列代表的会员的姓名，第 2 列代表年龄，第 3 列代表体重，第 4~6 列代表男性会员的三围尺寸，第 7~9 列代表女性会员的三围尺寸。

了解含义以后，我们再看下中间部分具体的数据，你可能会想，这些数据怎么这么「脏乱差」啊，有很多值是空的（NaN），还有空行的情况。

是的，这还仅仅是一家商店的部分会员数据，我们一眼看过去就能发现一些问题。日常工作中的数据业务会复杂很多，通常我们要统计更多的数据维度，比如 100 个指标，数据量通常都是超过 TB、EB 级别的，所以整个数据分析的处理难度是呈指数级增加的。这个时候，仅仅通过肉眼就很难找到问题所在了。

我举了这样一个简单的例子，带你理解在数据分析之前为什么要有数据清洗这个重要的准备工作。有经验的数据分析师都知道，好的数据分析师必定是一名数据清洗高手，要知道在整个数据分析过程中，不论是在时间还是功夫上，数据清洗大概都占到了 80%。

数据质量的准则

在上面这个服装店会员数据的案例中，一看到这些数据，你肯定能发现几个问题。你是不是想知道，有没有一些准则来规范这些数据的质量呢？

准则肯定是有的。不过如果数据存在七八种甚至更多的问题，我们很难将这些规则都记住。有研究说一个人的短期记忆，最多可以记住 7 条内容或信息，超过 7 条就记不住了。而数据清洗要解决的问题，远不止 7 条，我们万一漏掉一项该怎么办呢？有没有一种方法，我们既可以很方便地记住，又能保证我们的数据得到很好的清洗，提升数据质量呢？

在这里，我将数据清洗规则总结为以下 4 个关键点，统一起来叫「完全合一」，下面我来解释下。

完整性：单条数据是否存在空值，统计的字段是否完善。

全面性：观察某一列的全部数值，比如在 Excel 表中，我们选中一列，可以看到该列的平均值、最大值、最小值。我们可以通过常识来判断该列是否有问题，比如：数据定义、单位标识、数值本身。

合法性：数据的类型、内容、大小的合法性。比如数据中存在非 ASCII 字符，性别存在了未知，年龄超过了 150 岁等。

唯一性：数据是否存在重复记录，因为数据通常来自不同渠道的汇总，重复的情况是常见的。行数据、列数据都需要是唯一的，比如一个人不能重复记录多次，且一个人的体重也不能在列指标中重复记录多次。

在很多数据挖掘的教学中，数据准则通常会列出来 7~8 项，在这里我们归类成了「完全合一」4 项准则，按照以上的原则，我们能解决数据清理中遇到的大部分问题，使得数据标准、干净、连续，为后续数据统计、数据挖掘做好准备。如果想要进一步优化数据质量，还需要在实际案例中灵活使用。

清洗数据，一一击破

了解了数据质量准则之后，我们针对上面服装店会员数据案例中的问题进行一一击破。

这里你就需要 Python 的 Pandas 工具了。这个工具我们之前介绍过。它是基于 NumPy 的工具，专门为解决数据分析任务而创建。Pandas 纳入了大量库，我们可以利用这些库高效地进行数据清理工作。

这里我补充说明一下，如果你对 Python 还不是很熟悉，但是很想从事数据挖掘、数据分析相关的工作，那么花一些时间和精力来学习一下 Python 是很有必要的。Python 拥有丰富的库，堪称数据挖掘利器。当然了，数据清洗的工具也还有很多，这里我们只是以 Pandas 为例，帮你应用数据清洗准则，带你更加直观地了解数据清洗到底是怎么回事儿。

下面，我们就依照「完全合一」的准则，使用 Pandas 来进行清洗。

1.  完整性

问题 1：缺失值

在数据中有些年龄、体重数值是缺失的，这往往是因为数据量较大，在过程中，有些数值没有采集到。通常我们可以采用以下三种方法：

删除：删除数据缺失的记录；

均值：使用当前列的均值；

高频：使用当前列出现频率最高的数据。

比如我们想对 df [‘Age’] 中缺失的数值用平均年龄进行填充，可以这样写：

df['Age'].fillna(df['Age'].mean(), inplace=True)


如果我们用最高频的数据进行填充，可以先通过 value_counts 获取 Age 字段最高频次 age_maxf，然后再对 Age 字段中缺失的数据用 age_maxf 进行填充：

age_maxf = train_features['Age'].value_counts().index[0]


train_features['Age'].fillna(age_maxf, inplace=True)


问题 2：空行

我们发现数据中有一个空行，除了 index 之外，全部的值都是 NaN。Pandas 的 read_csv () 并没有可选参数来忽略空行，这样，我们就需要在数据被读入之后再使用 dropna () 进行处理，删除空行。

# 删除全空的行

df.dropna(how='all',inplace=True) 


2.  全面性

问题：列数据的单位不统一

观察 weight 列的数值，我们能发现 weight 列的单位不统一。有的单位是千克（kgs），有的单位是磅（lbs）。

这里我使用千克作为统一的度量单位，将磅（lbs）转化为千克（kgs）：

# 获取 weight 数据列中单位为 lbs 的数据

rows_with_lbs = df['weight'].str.contains('lbs').fillna(False)


print df[rows_with_lbs]


# 将 lbs 转换为 kgs, 2.2lbs=1kgs

for i,lbs_row in df[rows_with_lbs].iterrows():


# 截取从头开始到倒数第三个字符之前，即去掉 lbs。

  weight = int(float(lbs_row['weight'][:-3])/2.2)


  df.at[i,'weight'] = '{}kgs'.format(weight) 


3.  合理性

问题：非 ASCII 字符

我们可以看到在数据集中 Firstname 和 Lastname 有一些非 ASCII 的字符。我们可以采用删除或者替换的方式来解决非 ASCII 问题，这里我们使用删除方法：

# 删除非 ASCII 字符

df['first_name'].replace({r'[^\x00-\x7F]+':''}, regex=True, inplace=True)


df['last_name'].replace({r'[^\x00-\x7F]+':''}, regex=True, inplace=True)


4.  唯一性

问题 1：一列有多个参数

在数据中不难发现，姓名列（Name）包含了两个参数 Firstname 和 Lastname。为了达到数据整洁目的，我们将 Name 列拆分成 Firstname 和 Lastname 两个字段。我们使用 Python 的 split 方法，str.split (expand=True)，将列表拆成新的列，再将原来的 Name 列删除。

# 切分名字，删除源数据列

df[['first_name','last_name']] = df['name'].str.split(expand=True)


df.drop('name', axis=1, inplace=True)


问题 2：重复数据

我们校验一下数据中是否存在重复记录。如果存在重复记录，就使用 Pandas 提供的 drop_duplicates () 来删除重复数据。

# 删除重复数据行

df.drop_duplicates(['first_name','last_name'],inplace=True)


这样，我们就将上面案例中的会员数据进行了清理，来看看清理之后的数据结果。怎么样？是不是又干净又标准？

养成数据审核的习惯

现在，你是不是能感受到数据问题不是小事，上面这个简单的例子里都有 6 处错误。所以我们常说，现实世界的数据是「肮脏的」，需要清洗。

第三方的数据要清洗，自有产品的数据，也需要数据清洗。比如美团自身做数据挖掘的时候，也需要去除爬虫抓取，作弊数据等。可以说没有高质量的数据，就没有高质量的数据挖掘，而数据清洗是高质量数据的一道保障。

当你从事这方面工作的时候，你会发现养成数据审核的习惯非常重要。而且越是优秀的数据挖掘人员，越会有「数据审核」的「职业病」。这就好比编辑非常在意文章中的错别字、语法一样。

数据的规范性，就像是你的作品一样，通过清洗之后，会变得非常干净、标准。当然了，这也是一门需要不断修炼的功夫。终有一天，你会进入这样一种境界：看一眼数据，差不多 7 秒钟的时间，就能知道这个数据是否存在问题。为了这一眼的功力，我们要做很多练习。

刚开始接触数据科学工作的时候，一定会觉得数据挖掘是件很酷、很有价值的事。确实如此，不过今天我还要告诉你，再酷炫的事也离不开基础性的工作，就像我们今天讲的数据清洗工作。对于这些基础性的工作，我们需要耐下性子，一个坑一个坑地去解决。

好了，最后我们来总结下今天的内容，你都收获了什么？

学习完今天的内容后，给你留个小作业吧。下面是一个美食数据，如果你拿到下面的数据，按照我们今天讲的准则，你能找到几点问题？如果你来清洗这些数据，你打算怎样清洗呢？

欢迎在留言区写下你的思考，如果你对今天「数据清洗」的内容还有疑问，也欢迎留言和我讨论。也欢迎点击「请朋友读」，把这篇文章分享给你的朋友或者同事。

unpreview


© 版权归极客邦科技所有，未经许可不得传播售卖。页面已增加防盗追踪，如有侵权极客邦将依法追究其法律责任。

大龙

由作者筛选后的优质留言将会公开显示，欢迎踊跃留言。

Command + Enter 发表

0/2000 字

提交留言

精选留言 (75)

third


自己不知道有没有什么好的工具，所以就把图片上一个一个敲进去了。

数据.csv 格式

链接：https://pan.baidu.com/s/1jNnUpntrlxFSubmna3HtXw

提取码：e9hc

作者回复：感谢分享

2019-02-05


Hot Heat


可以给个样例数据的链接吗？自己动手操作一下

2019-01-07


wonderland


一、首先按照所讲的数据质量准则，数据存在的问题有：

1."完整性" 问题：数据有缺失，在 ounces 列的第三行存在缺失值

处理办法：可以用该列的平均值来填充此缺失值

2.「全面性」问题：food 列的值大小写不统一

处理办法：统一改为小写

3.「合理性」问题：某一行的 ounces 值出现负值

处理办法：将该条数据记录删除

4.「唯一性」问题：food 列大小写统一后会出现同名现象，

处理办法：需要将 food 列和 animal 列值均相同的数据记录进行合并到同一天记录中国

作者回复: Good Job

2019-01-10


滢

原始数据链接：https://github.com/onlyAngelia/Read-Mark/blob/master/ 数据分析 /geekTime/data/accountMessage.xlsx （课程中讲解原始数据 - 点击 view Raw 即可下载）

课后练习原始数据: https://github.com/onlyAngelia/Read-Mark/blob/master/ 数据分析 /geekTime/data/foodInformation.xlsx （点击 View Raw 下载）

作者回复：滢同学很好的分享！

2019-04-11


aof


这些东西，大家都一定要上手去实现一遍。最简单的就是，搞一个文本，把这些数据放进去，用 Python 读这个文本，转成 dataframe，把老师讲的那些清洗相关的 API 都一个一个试一下，才会有体会，光看一遍真的没啥用的！

现在只是很少的几十条数据，等你真正去搞那些上亿的数据的时候，就知道核对数据是个多么复杂的事情了……

作者回复：对的 一定要自己模拟操作下

2019-01-07


晨星

import pandas as pd


"""利用 Pandas 清洗美食数据"""

# 读取 csv 文件

df = pd.read_csv("c.csv")


df ['food'] = df ['food'].str.lower () # 统一为小写字母

df.dropna (inplace=True) # 删除数据缺失的记录

df ['ounces'] = df ['ounces'].apply (lambda a: abs (a)) # 负值不合法，取绝对值

# 查找 food 重复的记录，分组求其平均值

d_rows = df[df['food'].duplicated(keep=False)]


g_items = d_rows.groupby('food').mean()


g_items['food'] = g_items.index


print(g_items)


# 遍历将重复 food 的平均值赋值给 df

for i, row in g_items.iterrows():


    df.loc[df.food == row.food, 'ounces'] = row.ounces


df.drop_duplicates (inplace=True) # 删除重复记录

df.index = range (len (df)) # 重设索引值

print(df)


作者回复: Good Job

2019-02-19


nrvna


jupyter notebook，python3


import pandas as pd


df = pd.read_csv("D://Data_for_sci/food.csv")


df.index


df


# Data cleaning for lowercase


df['food'] = df['food'].str.lower()


df


# Delet NaN


df = df.dropna()


df.index = range(len(df)) # reset index


df


# Get bacon's mean value and delet second one


df.loc[0,'ounces'] = df[df['food'].isin(['bacon'])].mean()['ounces']


df.drop(df.index[4],inplace=True)


df.index = range(len(df)) # reset index


df


#Get pastrami's mean value and delet second one


df.loc[2,'ounces'] = df[df['food'].isin(['pastrami'])].mean()['ounces']


df.drop(df.index[4],inplace=True)


df.index = range(len(df)) # reset index


df


2019-01-09


Jbin


练习题中：

1、food 列中出现大小写不同的情况。根据实际，如果大小写不同的两个数据代表的产品不同，则不改变，否则统一改为小写

2、food 列 bacon 出现了三次，但是有两次是有正确数据，不能通过 food 去重。

3、ounces 列，去除空值行。根据实际数据来源以及分析目的，是否可能有负的情况，判断是否去除 - 3 行

以上为个人的一些想法，有考虑不周的地方希望老师可以指导。

2019-01-07


上官

weight = int(float((lbs_row['weight'][:-3])/2.2)


老师好，这行代码中 [：-3] 的作用是什么啊？

作者回复：截取从头开始到倒数第三个字符之前，即去掉 lbs。

2019-01-08


auroroa


最大的问题是不是没把数据的来源和目的描述清楚？😄

2019-01-07


桃园悠然在

我的理解，不能对 food 列简单去重吧，而是规范 ounces 列数据后汇总或者保持原样，这可能使厨房食材消耗记录。数据清洗还是要结合完全合一 + 业务含义。

2019-01-07


滢

觉得完全合一原则挺好，不过有些操作顺序是不是得更改一下，比如数值补全要在删除全空行之后，否则在补全的时候全空行也会补全。接下来总结在清洗过程中的问题：（1） 不知道 Python2 执行情况如何，在用 Python3 进行数据清理的时候，对于女性三围数据补全的时候因为列中有空字符的存在，会提示‘must str not int’, 需要自己过滤含有数值的有效数据进行 mean () 计算。(2) 生成的新列一般会自动补到后面，但 first_name,last_name 需要在第一列和第二列，所以要进行列移动或列交换。(3) 在删除数据之后默认加载的索引会出现问题，需要自己更新索引

作者回复：不错的分享

2019-04-11


爱做梦的咸鱼

建议老师涉及到数据集练习的可以把数据放在 github 上，方便我们做同步练习。

2019-01-29


程序员小熊猫

将磅（lbs）转化为千克（kgs）：

感觉这个地方写复杂了，直接用正则表达式替换就行了

df['Weight'].replace('lbs$', 'kgs', regex=True, inplace=True)


2019-01-19


周飞

完整性：ounces 列数据中存在 NAN

全面性：food 列数据中存在大小写不一致问题

合法性：ounces 列数据存在负值

唯一性：food 列数据存在重复

# -*- coding: utf-8 -*


import pandas as pd


import numpy as np


from pandas import Series, DataFrame


df = pd.read_csv('./fooddata.csv')


# 把 ounces 列中的 NaN 替换为平均值

df['ounces'].fillna(df['ounces'].mean(), inplace=True)


# 把 food 列中的大写字母全部转换为小写

df['food'] = df['food'].str.lower()


# 把 ounces 列中的负数转化为正数

df['ounces']= df['ounces'].apply(lambda x: abs(x))


#删除 food 列中的重复值

df.drop_duplicates('food',inplace=True)


print (df)


作者回复: Good Job

2019-01-12


王彬成

以下为对文中的案例进行编码操作，有三个问题请教

1、重量【‘weight’】一列的数据，如何利用平均值进行填充，因为该列是字符类型，无法求平均。目前采用高频数据填充

2、Pink Panther 用户的三围数据如何填充？，我想利用对应性别的平均值填充

3、案例中，后 6 列不显示‘NaN’，是因为填充列‘空格’吗？

_____________________


import pandas as pd


## 导入数据

df=pd.read_csv (' 第 11 节数据.csv')

## 重命名列名 columns

df.rename(columns={'0':'Number','1':'Name','2':'Age','3':'Weight','4':'m0006','5':'m0612','6':'m1218'


                  ,'7':'f0006','8':'f0612','9':'f1218'},inplace=True)


#2. 全面性

#列数据统一单位

# 获取 weight 数据列中单位为 lbs 的数据

rows_with_lbs=df['Weight'].str.contains('lbs').fillna(False)


print(df[rows_with_lbs])


# 将 lbs 转换为 kgs，2.2lbs=1kgs

for i,lbs_row in df[rows_with_lbs].iterrows():


# 截取从头开始到倒数第三个字符之前，即去掉 lbs

    weight=int(float(lbs_row['Weight'][:-3])/2.2)


    df.at[i,'Weight']='{}kgs'.format(weight)


#1. 完整性

# 删除全空的行

df.dropna(how='all',inplace=True)


## 缺失值补充方式一

## df [‘Age’] 中缺失的数值用平均年龄进行填充

#df['Age'].fillna(df['Age'].mean(),inplace=True)


## 缺失值补充方式二

## 使用 Age 一列高频数据进行填充

age_maxf=df['Age'].value_counts().index[0]


df['Age'].fillna(age_maxf,inplace=True)


## 使用 Weight 一列高频数据进行填充

weight_maxf=df['Weight'].value_counts().index[0]


df['Weight'].fillna(weight_maxf,inplace=True)


#4. 唯一性

#Name 拆分为 firstname 和 lastname

#切分名字，删除源数据列

df[['first_name','last_name']]=df['Name'].str.split(expand=True)


df.drop('Name',axis=1,inplace=True)


# 移动 first_name 和 last_name 这俩列

first_name=df.pop('first_name')


df.insert(1,'first_name',first_name)


last_name=df.pop('last_name')


df.insert(2,'last_name',last_name)


#删除重复数据行

df.drop_duplicates(['first_name','last_name'],inplace=True)


#3. 合理性

# 删除非 ASCII 字符

df['first_name'].replace({r'[^\x00-\x7F]+':''},regex=True,inplace=True)


df['last_name'].replace({r'[^\x00-\x7F]+':''},regex=True,inplace=True)


df


2019-02-12


lingmacker


import pandas as pd


def wash_data():


    data = pd.read_excel("./data/data.xlsx")


data ["food"] = data ["food"].str.capitalize () # 首字母大写

    data.fillna(0, inplace=True)


data.drop_duplicates ("food", inplace=True) # 删除重复行

    


    data.to_excel("./data/re_data.xlsx")


if __name__ == '__main__':


    wash_data()


是不是清洗得太简单了。。。

2019-01-17


Chino


有一个问题 就是代码最后那里 to_excel 如果参数的路径是指定的那种 就会报错显示 filenotfound 搜了很久都没找到是什么原因 求解

另外感觉这一讲有好多点都没讲深入呀 下面代码是对课程中的样例进行清洗 感觉只能做到几小点了。特别是在填充 nan 值的时候 一开始想着遍历每一个 nan 值 然后再特判列的类型进行填充的。但是发现三围那里有个大问题 按理说三围应该是 int 类型 但是因为有 - 这个东西的存在 搞的三围是 object 类型 一开始赋值的时候报错提示需要 str 后来想把列的类型转换成 int 也失败了 还有好多地方都卡着了...

import pandas as pd


import numpy as np


from pandas import Series, DataFrame


data = DataFrame(pd.read_excel('~/Desktop/data.xlsx'))


print(data)


# 更改列名

data.rename (columns={0:' 序号 ',1:' 姓名 ',2:' 年龄 ',3:' 体重 ',4:' 男三围 1',5:' 男三围 2',6:' 男三围 3',7:' 女三围 1',8:' 女三围 2',9:' 女三围 3'},inplace = True)

# 去掉重复行

data = data.drop_duplicates()


# 1. 完整性

# 填充缺失值

col = data.columns.values.tolist()


row = data._stat_axis.values.tolist()


# 先把姓名的数据类型改成字符串

data [' 姓名 '] = data [' 姓名 '].astype ('str')

# 1.1 先清除空行

data.dropna(how = 'all', inplace = True)


# 1.2 填充缺失值

age_maxf = data [' 年龄 '].value_counts ().index [0]

# 以年龄频率最大值来填充

data [' 年龄 '].fillna (age_maxf, inplace=True)

# 2. 全面性

# 把体重单位为 lbs 的转化为 kgs 2.2lbs = 1kgs

# 把所有体重单位为 lbs 的记录存放在一起 (如果体重是 nan 则不要)

rows_with_lbs = data [' 体重 '].str.contains ('lbs').fillna (False)

for i,lbs_row in data[rows_with_lbs].iterrows():


weight = int (float (lbs_row [' 体重 '][:-3]) / 2.2)

# 第一个参数是 y 坐标 (竖) 第二个参数是 x 坐标 (横)

data.at [i,' 体重 '] = '{} kgs'.format (weight)

print(data)


# 把清洗后的数据输出

data.to_excel('CleanData.xlsx')


# 会报错

# data.to_excel('~/Desktop/CleanData.xlsx')


作者回复: Good Job!

2019-01-17


北方

#!/usr/bin/env python


# -*- coding:utf8 -*-


# __author__ = ' 北方姆 Q'

# __datetime__ = 2019/1/11 15:53


import pandas as pd


# 导入

df = pd.read_csv("./s11.csv")


# 去除完全的空行

df.dropna(how='all', inplace=True)


# 食物名切分并去掉原本列

df[["first_name", "last_name"]] = df["food"].str.split(expand=True)


df.drop("food", axis=1, inplace=True)


# 名称首字母大写

df["first_name"] = df["first_name"].str.capitalize()


df["last_name"] = df["last_name"].str.capitalize()


# 以食物名为标准去重

df.drop_duplicates(["first_name", "last_name"], inplace=True)


print(df)


作者回复: Good Job

2019-01-11


奋斗

老师你好！我是爬虫新手，在为机器翻译提供语料，爬取完数据很头疼，文本数据里有很多问题，老师针对文本类的数据怎么处理好那，pandas 适用吗？谢谢了

2019-01-07


Tommy


脚本看不全啊

2019-01-07


qinggeouye


https://github.com/qinggeouye/GeekTime/blob/master/DataAnalysis/11_data_clean.py


import pandas as pd


# 读取数据

data_init = pd.read_csv("./11_clothingStoreMembers.csv")


# 清洗数据

# 删除 '\t' 列，读取 csv 文件多了一列

data_init.drop(columns='\t', inplace=True)


# 重命名列名

data_init.rename(


    columns={"0": "SEQ", "1": "NAME", "2": "AGE", "3": "WEIGHT", "4": "BUST_M", "5": "WAIST_M", "6": "HIP_M",


             "7": "BUST_F", "8": "WAIST_F", "9": "HIP_F"}, inplace=True)


print(data_init)


# 1、完整性

# 删除空行

data_init.dropna(how='all', inplace=True)


# 4、唯一性

# 一列多个参数切分

data_init[["FIRST_NAME", "LAST_NAME"]] = data_init["NAME"].str.split(expand=True)


data_init.drop("NAME", axis=1, inplace=True)


# 删除重复数据

data_init.drop_duplicates(["FIRST_NAME", "LAST_NAME"], inplace=True)


# 2、全面性

# 列数据单位统一，体重 WEIGHT 单位统一（lbs 英镑，kg 千克）

rows_with_lbs = data_init["WEIGHT"].str.contains("lbs").fillna(False)


print(rows_with_lbs)


# lbs 转为 kg

for i, lbs_row in data_init[rows_with_lbs].iterrows():


    weight = int(float(lbs_row["WEIGHT"][:-3]) / 2.2)


    data_init.at[i, "WEIGHT"] = "{}kgs".format(weight)


print(data_init)


# 3、合理性

# 非 ASCII 字符转换，这里删除处理

data_init["FIRST_NAME"].replace({r'[^\x00-\x7F]+': ''}, regex=True, inplace=True)


data_init["LAST_NAME"].replace({r'[^\x00-\x7F]+': ''}, regex=True, inplace=True)


# 1、完整性

# 补充缺失值 - 均值补充

data_init["AGE"].fillna(data_init["AGE"].mean(), inplace=True)


# 体重先去掉 kgs 的单位符号

data_init ["WEIGHT"].replace ('kgs$', '', regex=True, inplace=True) # 不带单位符号 kgs

data_init["WEIGHT"] = data_init["WEIGHT"].astype('float')


data_init["WEIGHT"].fillna(data_init["WEIGHT"].mean(), inplace=True)


data_init.replace ('-', 0, regex=True, inplace=True) # 读取的 csv 数据多了 '-'

data_init["WAIST_F"] = data_init["WAIST_F"].astype('float')


data_init["WAIST_F"].fillna(data_init["WAIST_F"].mean(), inplace=True)


# 用最高频的数据填充

age_max_freq = data_init["AGE"].value_counts().index[0]


print(age_max_freq)


data_init["AGE"].fillna(age_max_freq, inplace=True)


print(data_init)


作者回复：不错 把代码放到 GitHub 上是很好的方式

2019-11-08


姜泮昌

在唯一性的最后一步，去重后还有重复数据啊，Huey McDuck 有两条相同数据，这个有问题呢

2019-05-04


一语中的

#pd 读取数据

df = pd.read_excel('testdata11.xlsx')


#1. 完整性，ounces 列 NA 值用平均值填充

df['ounces'].fillna(df['ounces'].mean(), inplace=True)


#2. 全面性，统一 food 列大小写

df['food'] = df['food'].str.lower()


#3. 合法性，ounces 列负值取绝对值

df['ounces'] = df['ounces'].apply(lambda x: abs(x))


#4. 唯一性.animal 列有重复值

df.drop_duplicates('food', inplace=True)


#5. 重新排序显示

df.reset_index(drop=True, inplace=True)


print(df)


作者回复: Good Job

2019-02-26


徐薛彪

## 首先样本数据集存在如下问题:

完整性: ounces 列存在空值

全面性：

合理性: ounces 列存在负值

唯一性: bacon 美食存在多条记录，并且名称存在大小写，无法唯一区分一个食物信息

## 根据以上问题，可以做的数据清洗为：

1. 过滤异常数据行 (删除 ounces 列为空和负值的行)

2. 统一 food 字段命名使用小写

## 问题的本质:

其实个人认为问题的本质是数据思维和数据标准，如果单纯的给这样一份原始数据给数据分析人员，不提供上下文信息，那其实数据分析是很难做出效果的。比如按照我的清洗规则 1 清洗完成后数据如下:

           food ounces animal


0 bacon 4.0 pig


1 pulled pork 3.0 pig


2 Pastrami 6.0 cow


3 corned beef 7.5 cow


4 Bacon 8.0 pig


5 honey ham 5.0 pig


6 nova lox 6.0 salmon


虽然有 bacon 和 Bacon 可能是两种食物，但也很容易产生数据歧义，因为食物没有唯一标识。因此在有限的上下文环境中，数据分析可以做到如下数据集。

清洗过后的数据:

           food ounces animal foodtag


0 Bacon 8.0 pig bacon


1 Pastrami 6.0 cow pastrami


2 bacon 4.0 pig bacon


3 corned beef 7.5 cow corned beef


4 honey ham 5.0 pig honey ham


5 nova lox 6.0 salmon nova lox


6 pulled pork 3.0 pig pulled pork


## 相关 codes (将以下代码直接黏贴，在 python3 环境中运行)

#!/usr/bin/env python


# env: python 3.X


import pandas as pd


import pandasql


data = {'food':['bacon','pulled pork','bacon','Pastrami','corned beef','Bacon','pastrami','honey ham','nova lox'],'ounces':[4.0,3.0,None,6.0,7.5,8.0,-3.0,5.0,6.0],'animal':['pig','pig','pig','cow','cow','pig','cow','pig','salmon']}


foods = pd.DataFrame(data)


print ("原始数据集:\n % s" % foods)

# 注意：因为上面的 data 是我自己构造的，数据类型是已知的，通常在处理数据前，应当先查看下数据结构，转换成统一数据类型

print ("检查数据集每列的数据类型:\n % s" % foods.dtypes)

# 删除空行

foods.dropna(axis=0,how='any',inplace=True)


# 使用 sql 方式直接过滤异常数据

filterFoods = lambda sql: pandasql.sqldf(sql,globals())


sql = 'select * from foods where ounces > 0'


foodsnew = filterFoods(sql)


print ("过滤掉异常数据行:\n % s" % foodsnew)

foodsnew['foodtag'] = foodsnew['food'].str.lower()


print ("统一食物命名规范:\n % s" % foodsnew)

# 最后可以使用 sql 进行按照 food 排序

sql2 = "select * from foodsnew order by food"


print ("清洗过后的数据:\n % s" % filterFoods (sql2))

2019-02-24


littlePerfect


import pandas as pd


df = pd.read_excel("E:\data_analys_work/food_data.xlsx")


# 1. 完整性问题：缺失值

# df ['ounces'].fillna (df ['ounces'].mean (),inplace=True) # 平均值

ounces_maxf = df ['ounces'].value_counts ().index [0] # 出现频率最高的值

df['ounces'].fillna(ounces_maxf, inplace=True)


# 2. 全面性问题：列首字母不统一

df['food'] = df['food'].str.title()


# 3. 合理性问题：列数据的单位不统一

df['ounces'] = df['ounces'].apply(lambda x: abs(x))


# 4. 唯一性问题：食物重复出现，求和合并，并删除多余行

# 没想出来.....

作者回复: Good Sharing

2019-02-18


草包雷

import pandas as pd


df = pd.read_csv("c://leijin//food.csv")


df.index


print (df)


df['ounces']= df['ounces'].apply(lambda x: abs(x))


df['ounces'].fillna(df['ounces'].mean(),inplace=True)


df['food'] = df['food'].str.lower()


df.index=range(len(df))


print (df)


2019-02-18


王彬成

## 作业

import pandas as pd


df=pd.read_csv (' 第 11 节数据 - 2.csv')

## food 一列，统一变为小写字母

df['food']=df['food'].str.lower()


## 删除重量为空值的一行

df=df.dropna()


## 把 ounces 列中负数转换为正数

df['ounces']=df['ounces'].apply(lambda x:abs(x))


#重新排列序号

df.index=range(len(df))


df


2019-02-12


王彬成

1、完整性：

在 ounces 一列，存在缺失值。处理步骤：删除，因为有重复值

2、全面性：

food 列首字母大小写不统一。处理步骤：利用 DataFrame.columns.str.lower () 全部换成小写

3、合法性：

在重量一列，存在负数。处理步骤，删除

4、唯一性：

在食品名称一列，统一大小写后，存在重复值。处理：求和合并

作者回复：整理的不错

2019-02-12


胖猫

#mac python3.6


import pandas as pd


df = pd.read_excel("./home_11.xlsx")


df.dropna(how="all", inplace=True)


df['food'] = df['food'].str.capitalize()


df['ounces'] = df['ounces'].apply(lambda x : abs(x))


df.fillna(df['ounces'].mean(), inplace=True)


df.drop_duplicates('food', inplace=True)


print(df)


df.to_excel("./home_11_washed.xlsx")


2019-01-22


Chino


作业

import pandas as pd


import numpy as py


from pandas import Series, DataFrame


data = DataFrame(pd.read_excel('~/Desktop/HomeworkData.xlsx'))


# 把食物的名字统一大小写

data['food'] = data['food'].str.title()


# 完整性

# 填充空值

data['ounces'].fillna(data['ounces'].mean(), inplace = True)


# 全面性

# 没有发现问题

# 合理性

# ounces 值应大于等于 0 负值取绝对值

data['ounces'][data['ounces'] < 0] = data['ounces'][data['ounces'] < 0] * -1


# 唯一性

# 清除食物名重复的数据

data.drop_duplicates('food',inplace = True)


data.to_excel('HomeworkCleanData.xlsx')


作者回复: Good Job!

2019-01-17


雨先生的晴天

老师 你好，按照您的方法我清理一下数据，有一些疑惑，希望能指正。

在 ‘服装店统计的会员数据’ 例子 最终清洗截图中，最后截图中，

No.1 为什么 Huey McDuck 出现了两次？

No.2 为什么 Donald Duck 的体重数据 没有转化成 Kgs?

我试着按照例子，自己做了一边，发现以下代码，之修改成功了 Huey McDuck 的体重数据，189lbs=85kg,

rows_with_lbs=df['weight'].str.contains('lbs').fillna(False)


print df[rows_with_lbs]


weight = int(float(lbs_row['weight'][:-3])/2.2)


df.at[i,'weight']='{}kgs'.format(weight)


望您指正

2019-01-14


caidy


根据完全合一对数据进行清洗

1. 把有缺省值的删掉

2. 重量需要是正值，把负数的取绝对值

3. 把名称哪一类都改为小写

4. 去除重复行，或者因为是食物，可以把同样名字的食物加起来合成一行

2019-01-07


Viola


df['Age'].fillna(df['Age'].mean(), inplace=True)


老师好，对于文中的例子，自己手动尝试了下，发现并不能运行成功，所以有个疑问，

在 'Age' 列数据清洗前，还存在 NAN，这样去计算平均值（df ['Age'].mean ())，可行吗？

另外有个建议：如果文中涉及到对数据的处理，可否把数据文件以附件或 github 链接的形式给出来，方便我们学习呢？

2019-01-07


跳跳

1. 从完整性考虑，有缺失值。可以填充平均值

2. 从全面性考虑，food 列首字母大小写不统一，建议规范

3. 从合理性考虑，存在负值，修改为正值

4. 从唯一性考虑有三个 bacon，但是 ounces 存在不一致，考虑去掉含 NAN 的列

2019-01-07


Alex 王伟健

https://mubu.com/doc/7maWZgX_Gg


有空值，重复值，错值

先删掉重复值，然后把盎司用绝对值都变成正数，平均 / 中位 / 众数填充空值

2019-01-07


李璇

如果数据很多可以用 info 看一下缺失情况

food 首字母大写

用 bacon 的均值填充 bacon 缺失值

将负数 ounce 列删除

import pandas as pd


import os


os.chdir('C:\\Users\\Violette\\Desktop\\')


data=pd.read_csv ('11 作业数据.csv',encoding='UTF-8')

data.info()


data.food.str.capitalize()


mean_ounce=data[(data['food']=='Bacon')&(data['ounces'].notnull())]['ounces'].mean()


data.loc[data['ounces'].isnull(),'ounces']=mean_ounce


data= data.drop(data[data.ounces < 0].index).reindex()


2020-01-17


苹果

# -*- coding: utf-8 -*-


import numpy


import pandas as pd


import os


os.chdir ('E:\\C-05 - 数据分析实战 \\01 基础篇 \\ 数据清理材料 ')

df = pd.read_excel('.\\accountMessage.xlsx')


df.drop(columns=[0,'\t'],inplace = True)


df.rename(columns={1:'name',2:'age',3:'weights',4:'m0006',5:'m0612',6:'m1218',7:'f0006',8:'f0612',9:'f1218'},inplace=True)


df['age'].fillna(df['age'].mean(),inplace=True)


#用体重的众数填入空值

df['weights'].fillna(df['weights'].value_counts().index[0] , inplace=True )


#删除所用空值

df.dropna(how='all',inplace=True)


rows_with_lbs = df['weights'].str.contains('lbs')


# print(df[rows_with_lbs].iterrows())


# exit()


#统一单位为千克

for i ,lbs_row in df[rows_with_lbs].iterrows():


    weight = int(float(lbs_row['weights'][:-3])/2.2)


    df.at[i,'weights'] = '{}kgs'.format(weight)


df.replace({r'[^\x00-\x7F]+':''},regex=True)


# df['f0612'].fillna(df['f0612'].mean())


df[['first_name','last_name']] = df['name'].str.split(expand=True)


df.drop('name',axis=1,inplace=True)


# df.drop(columns=['name'],inplace=True)


df.drop_duplicates(['first_name','last_name'],inplace=True)


df.drop(index=[8],inplace=True)


columns = list(df)


columns.insert(0,columns.pop(columns.index('last_name')))


columns.insert(0,columns.pop(columns.index('first_name')))


df = df.ix[:,columns]


#f0612 用平均数填充空值

f0612_rows = df['f0612'].str.isalnum().fillna(True)


f0612_mean = df[f0612_rows]['f0612'].mean()


df['f0612'].fillna(f0612_mean,inplace=True)


df.replace('-','',inplace=True)


df.reindex(range(df.shape[0]),method = 'bfill')


print (df)


df.to_excel('.\\new_Message.xlsx')


2020-01-14


Ling


# jupyter notebook


# python 3


import pandas as pd


columns = ["food","ounces","animal"]


value = {"food":["bacon","pulled pork","bacon","Pastrami","corned beef","Bacon","pastrami","honey ham","nova lox"],


        "ounces":[4.0,3.0,None,6.0,7.5,8.0,-3.0,5.0,6.0],


        "animal":["pig","pig","pig","cow","cow","pig","cow","pig","salmon"]}


df = pd.DataFrame(value,columns=columns)


# 删除负值

df = df[df["ounces"] > 0]


# 使用均值填充空值

df.fillna(df.ounces.mean())


# food 列进行大小写转化

df.food = df.food.str.title()


# 一致化

df = pd.DataFrame(df.groupby(by=["food","animal"])["ounces"].sum())


2019-11-16


卢嘉敏

文字的缺失该如何实现数据清洗

作者回复：文字相对数值就比较困难了，除非是离散变量，比如「男」「女」这种的，当离散变量的可选项比较少的情况下，可以按照高频值的方式进行补全，当然你也可以根据那条数据自身的情况，选择适合的补全方式

2019-09-17


杨陆伟

微服务架构提供数据的微服务中也可以吸纳数据清洗的思想

作者回复：赞 不过数据清洗通常是个反复的过程，也就是清洗了一部分数据之后重新检查，然后再找到新的问题继续清洗，所以数据量大的时候，就像找 Bug 一样，花费的时间也很多

2019-08-21


杨宝强

data = {


        "ID": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, None, 9.0, 10.0],


        "name": ['Mickey Mouse', 'Donald Dunck', 'Mini Mouse', 'Scrooge McDuck', 'Pink Panther', 'Huey McDuck', 'Dewey McDuck', 'Scoopy Doo', None, 'Huey McDuck', 'Louie McDuck'],


        "age": [56.0, 34.0, 16.0, None, 54.0, 52.0, 19.0, 32.0, None, 52.0, 12.0],


        "weight": ['70kgs', '154.89lbs', None, '78kgs', '198.658lbs', '189lbs', '56kgs', '78kgs', None, '189lbs', '45kgs'],


        "m0006": [72.0, None, None, 78, None, None, None, 78.0, None, None, None],


        "m0612": [69, None, None, 79, None, None, None, 76, None, None, None],


        "m1218": [71.0, None, None, 72, None, None, None, 75.0, None, None, None],


        "f0006": [None, 85, 65, None, 69, 68, 71, None, None, 68, 92],


        "f0612": [None, 84, 69, None, None, 75, 78, None, None, 75, 95],


        "f1218": [None, 76, 72, None, 75, 72, 75, None, None, 72, 87]


}


作者回复: Good Job

2019-08-16


噼里啪啦啪啦噼里噼里啪啦

#coding = utf-8


'''


下面是对肉类数据进行清洗的练习

我们看到数据，有以下需要关注的：

1. 食物名称中首字母有大写有小写

2. 有很多重复数据，需要合并

'''


import pandas


import pandas as pd


from pandas import Series, DataFrame


df = DataFrame(pd.read_excel(r'F:\Work\open_courses\Data_Analysis_by_python\clean_data\foodInformation.xlsx'))


print(df)


print('---------------------------------------------')


'''


首先将食物名称调整成统一的大小写

'''


df['food'] = df['food'].str.capitalize()


print(df)


print('---------------------------------------------')


'''


找到相同的项，对数据进行合并

1. 首先针对 food 对 df 进行分组，并进行数据合并

2. 对分组后的数据进行重新 DataFrame 转换，形成新的 DataFrame

3. 对原来的 dataframe 进行处理，去重 + 去除没有意义的列

4. 两个 dataframe 合并

'''


#对数据进行分组

groupbyObj = df.groupby(df['food']).sum()


print(groupbyObj)


print('---------------------------------------------')


#对分组后的数据进行 DataFrame 化

new_df = groupbyObj.reset_index()


print(new_df)


print('---------------------------------------------')


#将原有的数据表进行去重，并删除 ounces 列

df.drop_duplicates(['food'], inplace=True)


df = df.reindex(range(df.shape[0]), method = 'bfill')


df.drop(columns = ['ounces'], inplace = True)


print(df)


print('---------------------------------------------')


#两个 DataFrame 合并

result = pd.merge(df, new_df, on = 'food')


print(result)


#打印

#result.to_csv(r'F:\Work\open_courses\Data_Analysis_by_python\clean_data\foodInformation_new.csv')


作者回复: Good Job

2019-08-09


别离

food ounces animal


0 bacon 4.0 pig


1 pulled pork 3.0 pig


2 bacon NaN pig


3 Pastrami 6.0 cow


4 corned beef 7.5 cow


5 Bacon 8.0 pig


6 pastrami -3.0 cow


7 honey ham 5.0 pig


8 nova lox 6.0 salmon


2019-08-08


王张

#_*_ coding:utf-8 _*_


import pandas as pd


data = {'ounces':[4.0,3.0,None,6.0,7.5,8.0,-3.0,5.0,6.0],'animal':['pig','pig','pig','cow','cow','pig','cow','pig','salmon']}


df=pd.DataFrame(data,index=['bacon','pulled pork','bacon','Pastrami','corned beef','bacon','Pastrami','honey ham','nova lox'],columns=['ounces','animal'])


print(df)


df.to_excel('data1.xlsx')


------------------------


你们要的数据

作者回复：多谢分享

2019-08-01


建强

1."food" 列数据大小写不统一，清洗方法：统一转换成小写。

2."ounces" 列数据有缺失值，用平均值填充。

3."ounces" 列有一行数据是 - 3，不合理数据，转换为正数。

4."food" 列中，值 ="bacon"、"pastrami" 的行有重复，可考虑把这些重复行进行合并处理。

以上是我能发现的几点问题，请老师指正。

作者回复: Good Job

2019-07-30


xqs42b


老师，有答案吗？？我想知道我的想法有没有啥问题！

2019-07-08


TONY


import pandas as pd


import numpy as np


df = pd.read_csv("./foods.csv")


df.columns = df.columns.str.strip () #去掉列名空格，否则影响后续对列的操作；

df ['food'] = df ['food'].str.strip () #去掉 food 列名称空格，否则影响对名称的操作；

df ['food'] = df ['food'].str.capitalize () # 对 food 名称首字母大写

df ['ounces'] = df ['ounces'].apply (lambda a: abs (a)) #对 ounces 列负值取绝对值

df.fillna (df.mean (),inplace = True) #在取绝对值后用列平均值填充 NaN 值

df


df.drop_duplicates ('food',inplace = True) #删除 food 列重复数据

df


df.to_csv ('./foods.csv') #将结果写回 csv

print(df)


作者回复: Good Job

2019-06-17


Kyle


根据完全合一的法则来说，那么完整性：ounces 列出现了缺失值，全面性：name 列名称命名大小写不统一，合法性：ounces 列出现了负值，唯一性：食物行出现了重复的值，不知道以哪行为主。

2019-05-29


忘矢

请问老师，上亿条记录的数据量也适合用 Python 来作清洗吗？

作者回复：也可以，有 4600 万条数据量，你也可以采用 Python 来处理，就需要用一些方法了，比如分 chunk（块）读取，然后操作

2019-05-08


冯德章

1. 数据清洗是数据分析的基础，数据准确性决定结果准确性。

2. 数据清洗占数据挖掘工作量 80，

3. 数据清洗，完全合一原则。

完整性，确保没有空值。

全面性，确认单位，数据质量

合法性，数据是否遵从自然规律。

唯一性，去除重复数据。

4. 数据结果检核习惯。

作者回复：总结不错

2019-04-28


翟小童

path = r"d:/exam.txt"


df = pd.read_csv(path, sep='\t')


df ['food'] = df ['food'].str.lower () # food 列转换为小写

print (df ['ounces'].isnull ().value_counts ())# 统计空得数量

df = df.dropna () # 删除空行

df.loc [df [df ['ounces'] <0].index, 'ounces'] = df [df ['food'].isin (['pastrami'])]['ounces'].mean () # 取平均值赋值给异常值

# print(df)


df.groupby ('food').mean () # 计算每种食物的平均值

df.drop_duplicates (['food', 'animal'], inplace=True)# 删除重复数据行

df.index = range(len(df))


2019-04-25


青石

#!usr/bin/python3


# -*- coding:utf-8 -*-


import os


import numpy as np


import pandas as pd


path = '/Users/albert.ming.xu/Downloads/11.data.xlsx'


new_path = os.path.splitext(path)[0] + '_new' + os.path.splitext(path)[1]


data_type = {'food': np.str, 'ounces': np.float, 'animal': np.str}


df = pd.read_excel(path, sheet_name=0, header=0, dtype=data_type)


# 统一 food 名称为小写字母

df['food'] = df['food'].str.lower()


# 单位取绝对值

df['ounces'] = df['ounces'].abs()


# 将 ounces 列的空行填充为平均值

df['ounces'].fillna(df['ounces'].mean(), inplace=True)


# 删除任意字段包含空值的数据

df.dropna(how='any', axis=0, inplace=True)


# 删除 food 字段出现重复的行，但 bacon 最后会存在三行数据，ounces 分别是 4.0、5.31、8.0，最后保留哪个数据更好是个问题

df.drop_duplicates('food', inplace=True)


df.to_excel(new_path, float_format="%.2f")


作者回复: Good Job

2019-04-13


滢

录入的原始数据链接：

关于练习：首先把这份数据当做食材消耗，单位为盎司（当然对业务的理解有多种）。（1）对于这份数据 NaN 值应该删除，不能进行补全；（2）如果是食材消耗，负数的存在也是合理的，不用对齐求绝对值（3）重复的原始数据不能删除，应该是汇总后再进行删除，以下是代码，Python3.6 IDLE

>>> import pandas as pd


>>> from pandas import Series,DataFrame


>>> df = DataFrame (pd.read_excel ('/Users/apple/Desktop/GitHubProject/Read mark / 数据分析 /geekTime/data/foodInformation.xlsx'))

>>> print (df)


          food ounces animal


0 bacon 4.0 pig


1 pulled port 3.0 pig


2 bacon NaN pig


3 Pastrami 6.0 cow


4 corned beef 7.5 cow


5 Bacon 8.0 pig


6 pastrami -3.0 cow


7 honey ham 5.0 pig


8 nova lox 6.0 salmon


#删除空值

>>> df.dropna(inplace=True)


#food 一栏全部换成小写

>>> df['food']=df['food'].str.lower()


#查找 food 重复记录，分组求其总和

>>> food_rows = df[df['food'].duplicated(keep=False)]


>>> food_items=food_rows.groupby('food').sum()


>>> food_items['food']=food_items.index


#数据赋值替换

>>> for i,row in food_items.iterrows():


df.loc[df['food']==row['food'],'ounces']=row['ounces']


#删除重复

>>> df.drop_duplicates(inplace=True)


#重新生成索引

>>> df.index=range(df.shape[0])


>>> print (df)


          food ounces animal


0 bacon 12.0 pig


1 pulled port 3.0 pig


2 pastrami 3.0 cow


3 corned beef 7.5 cow


4 honey ham 5.0 pig


5 nova lox 6.0 salmon


作者回复: Good Job

2019-04-11


FeiFei


80％时间需要在数据清洗上。

担心公司不能给予那么多时间来做这件事。

作者回复：哈哈哈 很好的分享

2019-04-09


『G YaQi』


import pandas as pd


from pandas import Series, DataFrame


# 读入数据

data = DataFrame(pd.read_excel('C:\\Users\\Administrator\\Desktop\\food.xlsx'))


# 删除异常值所在行

rst = data[data['ounces'] < 0].index.tolist()


data.drop(index=rst, inplace=True)


# 食物名字小写

data['food'] = data['food'].str.lower()


# 以均值填充空值

data['ounces'].fillna(int(data['ounces'].mean()), inplace=True)


# 重置索引

data = data.reset_index(drop=True)


print(data)


作者回复: Good Job

2019-03-21


王

第三个规则，合值得是合理性还是合法性呢？

作者回复：一个含义

2019-02-28


方人其

pandas 适合多大的数据量清洗？十万级别？百万级别？千万级别？亿级别？？

作者回复：千万级别是可以的，前提是你可以采用分块的方式来操作，chunksize

2019-01-30


圆圆的大食客

import pandas as pd


df = pd.read_csv("D:\\Learning\\Data 45 Lessons\practice11.csv")


# fill NaN as average


df['ounces'].fillna(df['ounces'].mean(), inplace = True)


#Delete blank line


df.dropna(how = "all", inplace = True)


#Capitalize food name


df['food']=df['food'].str.capitalize()


#Change negative value to positive value


df['ounces']= df['ounces'].apply(lambda x: abs(x))


#Delete duplicates


df.drop_duplicates(['food'], inplace=True)


print (df)


2019-01-22


李沛欣

完整性，全面性，合理性，一致性。

现实世界的数据是肮脏的，需要我们把它们清洗干净。

除了简单的处理外，还可以用 pandas。

周末可以键盘敲起来了。

作者回复：加油～！

2019-01-18


LI.T.F


只发现了三点错误：

1. 食物名称有的首字母大写，有的没有大写

2. 盅司这一列中有 NAN

3. 盅司列有负数

数据清理：

1. 食物列首字母大写

2. 盅司列用平均数代替 NAN

3. 盅司列的负数变成正数

作者回复: Good Job~

2019-01-17


Mark


使用 py 做数据清洗，数据量大小有限制吗？tb 级也是全部加载到内存中处理？还是使用大数据技术？选择使用 py 是因为他有数据处理的库，并且是方便处理少量数据的工具，是吗？

作者回复: Python 也可以处理大量的数据，选择 Python 是因为用的人多，而且工具包完善，走前人走的路，不需要重复造轮子

2019-01-16


lee4


重点还要看数据使用的场景和目的。

根据现有信息，有可能采取以下行动：

1. 将 food 一列内容写法统一，例如大小写

2. food 一列不宜去重，可能需要另行建立 index

3. ounces 一列将 Nan 去除，或填充为 0 并另行建立 indicator 标识数据缺失

4. ounces 一列将负值去除，或取绝对值

5. animal 一列可以考虑将分类转换为数值型 indicator

作者回复: Good Job

2019-01-15


夏落若

用 scrapy 爬取了安居客武汉租房的数据 2950 条，有需要的去下载：https://download.csdn.net/download/weidan0302/10905156，如果需要代码，可以联系我

2019-01-09


讲究人

老师，讲义中的数据有提供下载地址吗？

2019-01-08


wolf|▍奇葩

客观说在极客买了好几个课程这个课程帮助最大，很好的帮我梳理了思路，能再快点看到新课就好了，陈老师加油～

2019-01-07


JingZ


#2019/1/7 Pandas 数据清理

假设场景是食品采购环境，根据「完全合一」原则，问题和清洗如下（代码实践）：

1、ounces 缺失值 -> NaN 用「0」替换

2、ounces 不合理值 -> -3 用「3」替换（考虑可能输入者的输入错误，采购多点比少点好，当然成本这儿分析时稍忽略下）

3、food 列数据大小写 -> 统一首字母为大写（感觉不需要像人名字那种分开两列）

4、animal 列感觉需要将同类放一起进行排序 ->cow/pig/salmon 的英语字母顺序

2019-01-07


梁林松

1，food 可以改为 food name

2, 倒数第 3 行 - 3.0 和正数第 3 行 NAN 需要去除。

3，统一大小写。

4，合并同类项（正数 1 ，3，6 行）（正数 4，7 行）。

5，把 ounce 和 animal 左右对调，让单位在最右端。

2019-01-07


auroroa


1、完整性问题，含有了 NA 值，因为有重复记录，根据数据来源描述可判断取同名称记录下的其他行该列不为空的数值

2、全面性问题，food 列首字母大小写不一致，可统一为小写

3、合理性问题，含有 onuces 为负值的记录，因为已有该名称食物数据，可剔除该行

4、唯一性问题，统一名称大小写后有同名食物，合并记录在满足数据合理性完整性条件下分别对各列取最优值

当然数据量少和数据量大的时候处理方式可能不一样

2019-01-07


浩然

删掉负值行，pig 的空值用 pig 的平均数填充

2019-01-07


梅破知春近

首先 food 列文本都改为小写，.lower ()，对 food 列去重，去重规则为第二列优先保留大于 0 的数值，优先保留非 NA 值，去重后检验是否还有 NA，填充 NA 值。

2019-01-07


无法言喻.

大小写 重复值 空值 分类数值化

有之前讲解的服装店会员的 csv 吗？

2019-01-07


Python


直接把负三，以及空值那两行删除，因为她们既是 food 的重复行，也是不合格的数据

2019-01-07


許敲敲

food 都小写后 去重，重量都取绝对值，再填充 NA 值。

2019-01-07


蜘蛛的梦呓

首先，统一列名，把列名都改为小写。

然后，异常处理，去掉不合常理的数值 (负数)，空值删除或用平均数填充。

最后，去重，删除重复列。

2019-01-07


收起评论




7592










